Homework1:

问题是这样的：设计并进行一个实验，以展示样本平均方法在非平稳问题中的局限性。

在之前的10-arm实验中，真实q*(a)是从N(0,1)采样而固定的，并探讨了ε-greedy策略，对比了三种ε值下的表现，进行了1000步训练。我复刻了这个实验，代码整理在了作业文件中。复刻实验的图像如下：

【实验2.2.图的图像】

这个实验表明ε=0.1在三种设置中是最优的。利用这个最优ε，我们继续深入探讨另一个问题：如果环境是nonstationary的，不同的Q估计方法会有什么区别呢？这次我们采用上述实验所证明的最优ε=0.1，以及更多的steps来设置testbed。

我们先来回顾一下要讨论的方法：
1、样本平均法

公式：
\begin{equation}
Q(a) = \frac{1}{N(a)} \sum_{i=1}^{N(a)} R_i
\end{equation}

 
直接存储所有奖励，然后计算均值。该方法需要存储所有过去的奖励值，计算量较大。

2、增量计算法

公式：
\begin{equation}
Q(a) \leftarrow Q(a) + \frac{1}{N(a)}(R - Q(a))
\end{equation}


这种方法其实就是样本平均法，只是对计算方法进行了优化。在本次试验中，我们直接采用增量计算法，这个和样本平均法（直接求均值）得到的结果是一样的，并且更加高效。

3、constant step-size, α=0.1,

\begin{equation}
Q(a) \leftarrow Q(a) + \alpha (R - Q(a))
\end{equation}

该方法更加关注最近的数据，在本次试验中，我们将探讨他能否如预期那样，快速适应q*(a)变化，并在nonstationary环境中表现更好。

因此，综上所述，我们的testbed将进行如下设置：

Bandit环境设置：

1、初始时所有arm的q*(a)相同，可以设置一个固定值或者初始化为0

2、 在每个step，所有的q*(a)进行随机游走：

q*(a)←q*(a)+N(0,0.01)

这样的话q*(a)就会不断变化，使得问题变成非平稳环境。

3、 当agent选择某个动作a时：

reward服从 R∼N(q\*(a),1)

这点延续了2.2实验，以一个模拟噪声的方式来呈现。

Agent设置：

1、 使用ε = 0.1的ε-greedy 策略

2、 对比样本平均法（增量计算）和constant step-size method

实验运行：

1、 每个实验运行10000steps

2、 运行2000次实验取平均，并记录：
    - 每一步的平均奖励（绘制曲线一）
    - 最优动作的选择率（绘制曲线二）

根据理论知识预期的结果：固定步长法能够更快适应q*(a)的变化，并在两个曲线中都表现更好。

按照上述实验设置，我编写了代码，运行结果如下：

【图组一】

由于上述图像难以通过点和线的形式区分，特别用滑动平均来展示整体趋势：

【图组二】

通过上述实验结果，我们可以看到：

Smoothed Average Reward vs Steps（左图）

橙色虚线（Constant Step-Size 
𝛼
=
0.1
α=0.1）的奖励更高，随着时间推移，奖励稳步增长。
蓝色实线（Sample-Average）增长较慢，并且后期收敛较低。
结论：固定步长（
𝛼
=
0.1
α=0.1）能更快适应非平稳环境，提高平均奖励。
Smoothed Optimal Action Selection vs Steps（右图）

橙色虚线（Constant Step-Size） 的最优动作选择率更高，最终稳定在 70% 左右。
蓝色实线（Sample-Average）表现较差，最终稳定在 40% 左右。
结论：固定步长（
𝛼
=
0.1
α=0.1）能够更快适应变化，使智能体选择最优动作的概率更高。


现在，通过这个实验结果，我们可以初步确信constant step-size的效果更好。

结合理论分析一下，为什么Constant Step-Size 更适合非平稳环境：

Sample-Average（样本平均法）的问题

1. sample-average会一直计算所有历史奖励的均值，所以如果环境是非平稳的（q*(a) 在变化），老数据会影响新决策。
学习速度慢，适应变化的能力较弱。
Constant Step-Size（固定步长 
𝛼
α）的优势

2. constant step-size只考虑最近的奖励，忽略过时的数据。
适合非平稳问题，能更快调整策略。

上述实验成功验证了 Sample-Average 方法不适用于非平稳环境，而 Constant Step-Size (
𝛼
=
0.1
α=0.1) 能更快适应变化，提高最终性能！


但是，如2.2实验那样，我们还可以进一步探索。我们可以继续探究，不同的 
α 值（比如 0.05, 0.2, 0.5），看看 步长不同会有什么影响。

此外，是否能改进样本平均法？样本平均法适应于平稳环境，能逐步收敛，但在非平稳环境中，适应变化较慢。它给予所有过去的数据相同的权重，即旧的数据影响力并不会随着时间而减少。在nonstaionary环境中，q*(a)会随着时间变化而变化，这导致Q值被过时的数据拖累。那么，如果我们对样本平均法进行修改，让其旧数据的影响随着时间的增加而减少，是否能让训练效果变得更好呢？

样本平均法的公式：

        $ Q_{t+1} = Q_t + \frac{1}{N_t} (R_t - Q_t) $ & 


随着时间的增加，让旧数据的影响减少，我们增加时间t因子：


        $ Q_{t+1} = (1 - \frac{1}{t}) Q_t + \frac{1}{t} R_t $ & 

这样，早期的数据随着时间的增加就会被适度遗忘掉。

我们用上面同样的方法，增加自适应步长进行测试：

【结果图】

结果显示，自适应步长效果非常差，可能原因是在t比较大的情况下，虽然旧的Q值影响减少了，但是新的Q值影响也很小，导致无法适应环境变化。而这里自适应步长比样本平均法的效果还要差，可能是因为步长衰减太快，导致新数据的影响甚至比样本平均还要小。

那么我们换个思路，既然要强调适应新的奖励，那么在样本平均法的基础上，采用限制性滑动窗口，也就是说，只考虑最近的N步的奖励，而忽略更久远的历史数据。

设N位考虑的最近的N步，则新的样本平均法（滑动窗口）直接计算公式为：

Q_t = \frac{1}{N} \sum_{i=t-N}^{t} R_i

增量更新公式为：

Q_t = Q_{t-1} + \frac{1}{N} \left( R_t - R_{t-N} \right)

其中Rt−N是窗口中最老的奖励，需要从均值中去掉。

经过实验，得到如下数据：

【图】


可以看到，滑动平均大大改善了样本平均，但依旧不及固定步长。

这表明，在nonstationary环境下，持续学习新数据比记忆过去数据更加重要。

最后，我们选取固定步长法，测试不同的α值的表现，来找到在该实验环境下最优的α。

在代码中，我设置了alpha_values = [0.01, 0.05, 0.1, 0.2, 0.5]，经过实验，得到如下结果：

【结果】

从实验结果来看，固定步长 α=0.1 确实表现最佳，既能快速适应环境变化，又不会像 α=0.5 那样受噪声影响太大，也不会像 α=0.01 那样适应得太慢。


