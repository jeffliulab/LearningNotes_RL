import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm # ç”¨äºè¿›åº¦æ¡ï¼Œå¯ä»¥åˆ é™¤ï¼Œå¦‚æœæ²¡æœ‰è¿™ä¸ªåº“ï¼Œè¯·å‚è§ä»£ç ä¸­çš„commentsï¼ŒæŠŠå¯¹åº”çš„é‚£ä¸€æ¡åˆ é™¤

# 10-è‡‚èµŒåšæœºç¯å¢ƒï¼ˆå¸¦éšæœºæ¸¸èµ°çš„éå¹³ç¨³ç¯å¢ƒï¼‰
class Bandit:
    def __init__(self, num_arms=10):
        self.num_arms = num_arms
        self.q_star = np.zeros(num_arms)  # åˆå§‹æ—¶æ‰€æœ‰ q*(a) è®¾ä¸º 0

    def get_reward(self, action):
        return np.random.normal(self.q_star[action], 1)  # N(q*(a), 1)

    def update_q_star(self):
        """ æ¯ä¸€æ­¥åï¼Œæ‰€æœ‰ q*(a) è¿›è¡Œéšæœºæ¸¸èµ° """
        self.q_star += np.random.normal(0, 0.01, self.num_arms)

    def optimal_action(self):
        """ è¿”å›å½“å‰ q*(a) æœ€é«˜çš„æ‹‰æ†ç´¢å¼• """
        return np.argmax(self.q_star)

# æ™ºèƒ½ä½“ï¼ˆÎµ-greedy ç­–ç•¥ï¼Œæ”¯æŒä¸¤ç§ Q ä¼°è®¡æ–¹æ³•ï¼‰
class Agent:
    def __init__(self, num_arms=10, epsilon=0.1, alpha=None):
        self.epsilon = epsilon
        self.alpha = alpha  # None ä»£è¡¨æ ·æœ¬å¹³å‡æ³•ï¼Œå›ºå®šæ•°å€¼ä»£è¡¨ Î±=0.1
        self.q_estimates = np.zeros(num_arms)  # ä¼°è®¡çš„ Q å€¼
        self.action_counts = np.zeros(num_arms)  # è®°å½•æ¯ä¸ªåŠ¨ä½œçš„é€‰æ‹©æ¬¡æ•°

    def select_action(self):
        """ Îµ-greedy é€‰æ‹©ç­–ç•¥ """
        if np.random.rand() < self.epsilon:
            return np.random.randint(len(self.q_estimates))  # 10% éšæœºæ¢ç´¢
        else:
            return np.argmax(self.q_estimates)  # 90% é€‰æ‹©å½“å‰æœ€ä¼˜åŠ¨ä½œ

    def update(self, action, reward):
        """ æ›´æ–° Q ä¼°è®¡å€¼ """
        self.action_counts[action] += 1
        if self.alpha is None:  # æ ·æœ¬å¹³å‡æ³•
            alpha = 1 / self.action_counts[action]
        else:  # å›ºå®šæ­¥é•¿ Î±=0.1
            alpha = self.alpha
        self.q_estimates[action] += alpha * (reward - self.q_estimates[action])

# è¿è¡Œå®éªŒ
def run_experiment(num_steps=10000, num_experiments=2000, epsilon=0.1, alpha=None):
    avg_rewards = np.zeros(num_steps)
    optimal_action_pct = np.zeros(num_steps)

    # for _ in range(num_experiments): å¦‚æœä¸æƒ³è¦è¿›åº¦æ¡ï¼Œå°±æŠŠä¸‹é¢çš„forå¾ªç¯æ”¹æˆè¿™ä¸€è¡Œ
    for _ in tqdm(range(num_experiments), desc="Running Experiments"):
        bandit = Bandit()
        agent = Agent(epsilon=epsilon, alpha=alpha)
        optimal_action = bandit.optimal_action()

        for step in range(num_steps):
            action = agent.select_action()
            reward = bandit.get_reward(action)
            agent.update(action, reward)
            bandit.update_q_star()  # q*(a) è¿›è¡Œéšæœºæ¸¸èµ°

            avg_rewards[step] += reward
            optimal_action_pct[step] += (action == bandit.optimal_action())

    avg_rewards /= num_experiments
    optimal_action_pct = (optimal_action_pct / num_experiments) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”
    return avg_rewards, optimal_action_pct

# è¿è¡Œå®éªŒï¼Œæ¯”è¾ƒä¸¤ç§ Q ä¼°è®¡æ–¹æ³•
num_steps = 10000
num_experiments = 2000
epsilon = 0.1

# 1ï¸âƒ£ æ ·æœ¬å¹³å‡æ³•
print("Sample Average - Incrementally Computed: ")
rewards_sample_avg, optimal_sample_avg = run_experiment(num_steps, num_experiments, epsilon, alpha=None)

# 2ï¸âƒ£ å›ºå®šæ­¥é•¿ Î±=0.1
print("Constant Step-Size: ")
rewards_const_alpha, optimal_const_alpha = run_experiment(num_steps, num_experiments, epsilon, alpha=0.1)

# ç»˜åˆ¶ç»“æœ
plt.figure(figsize=(12, 5))

# å¹³å‡å¥–åŠ±æ›²çº¿
plt.subplot(1, 2, 1)
plt.plot(rewards_sample_avg, linestyle="-", marker="o", markersize=3, linewidth=1.5, label='Sample-Average')
plt.plot(rewards_const_alpha, linestyle="--", marker="s", markersize=3, linewidth=1.5, label='Constant Step-Size Î±=0.1')
plt.xlabel("Steps")
plt.ylabel("Average Reward")
plt.legend()
plt.title("Average Reward vs Steps")

# æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡æ›²çº¿
plt.subplot(1, 2, 2)
plt.plot(optimal_sample_avg, linestyle="-.", marker="^", markersize=3, linewidth=1.5, label='Sample-Average')
plt.plot(optimal_const_alpha, linestyle=":", marker="x", markersize=3, linewidth=1.5, label='Constant Step-Size Î±=0.1')
plt.xlabel("Steps")
plt.ylabel("% Optimal Action")
plt.legend()
plt.title("Optimal Action Selection vs Steps")

plt.show()


## ç”±äºä¸Šè¿°å›¾åƒéš¾ä»¥é€šè¿‡ç‚¹å’Œçº¿çš„å½¢å¼åŒºåˆ†ï¼Œç‰¹åˆ«ç”¨æ»‘åŠ¨å¹³å‡æ¥å±•ç¤ºæ•´ä½“è¶‹åŠ¿ï¼š

# è¿™é‡Œç”¨æ»‘åŠ¨å¹³å‡å±•ç¤ºæ–°çš„ä¸¤å¼ å›¾

# ğŸ¨ **æ–°å›¾ï¼šä½¿ç”¨æ»‘åŠ¨å¹³å‡å±•ç¤ºæ›´å¹³æ»‘çš„è¶‹åŠ¿**

# è®¡ç®—æ»‘åŠ¨å¹³å‡ï¼ˆMoving Averageï¼‰
def moving_average(data, window_size=100):
    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')

# x è½´å¯¹é½
x_smooth = np.arange(len(moving_average(rewards_sample_avg)))

plt.figure(figsize=(12, 5))

# ğŸ“Œ å¹³å‡å¥–åŠ±æ›²çº¿ï¼ˆå¹³æ»‘ç‰ˆï¼‰
plt.subplot(1, 2, 1)
plt.plot(x_smooth, moving_average(rewards_sample_avg), linestyle="-", linewidth=2.5, color="blue", label='Sample-Average')
plt.plot(x_smooth, moving_average(rewards_const_alpha), linestyle=":", linewidth=2.5, color="orange", label='Constant Step-Size Î±=0.1')
plt.gca().lines[-1].set_dashes((3, 6))  # è®¾ç½®ç‚¹çŠ¶è™šçº¿ï¼ˆé—´éš”è¾ƒå¤§ï¼‰
plt.xlabel("Steps")
plt.ylabel("Smoothed Average Reward")
plt.legend()
plt.title("Smoothed Average Reward vs Steps")
plt.grid(True, linestyle="--", alpha=0.6)  # åŠ ç½‘æ ¼çº¿

# ğŸ“Œ æœ€ä¼˜åŠ¨ä½œé€‰æ‹©ç‡æ›²çº¿ï¼ˆå¹³æ»‘ç‰ˆï¼‰
plt.subplot(1, 2, 2)
plt.plot(x_smooth, moving_average(optimal_sample_avg), linestyle="-", linewidth=2.5, color="blue", label='Sample-Average')
plt.plot(x_smooth, moving_average(optimal_const_alpha), linestyle=":", linewidth=2.5, color="orange", label='Constant Step-Size Î±=0.1')
plt.gca().lines[-1].set_dashes((3, 6))  # è®¾ç½®ç‚¹çŠ¶è™šçº¿ï¼ˆé—´éš”è¾ƒå¤§ï¼‰
plt.xlabel("Steps")
plt.ylabel("Smoothed % Optimal Action")
plt.legend()
plt.title("Smoothed Optimal Action Selection vs Steps")
plt.grid(True, linestyle="--", alpha=0.6)  # åŠ ç½‘æ ¼çº¿

plt.show()
